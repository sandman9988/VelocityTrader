#!/usr/bin/env python3
"""
ProjectQuantum Docker Backtesting Runner
Executes backtests in Docker environment with performance monitoring and feedback loop

Author: ProjectQuantum CI
Version: 1.216
"""

import os
import sys
import json
import time
import argparse
import subprocess
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple


class BacktestRunner:
    """Manages backtest execution and result collection"""
    
    def __init__(self, workspace: Path, results_dir: Path):
        self.workspace = workspace
        self.results_dir = results_dir
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        # Backtest configuration
        self.default_config = {
            "symbol": "EURUSD",
            "timeframe": "H1",
            "start_date": "2023.01.01",
            "end_date": "2023.12.31",
            "initial_deposit": 10000,
            "leverage": 100,
            "optimization_mode": False,
            "visual_mode": False,
            "forward_testing": False
        }
    
    def load_config(self, config_file: Optional[Path] = None) -> Dict[str, Any]:
        """Load backtest configuration from file or use defaults"""
        if config_file and config_file.exists():
            with open(config_file, 'r') as f:
                config = json.load(f)
            # Merge with defaults
            return {**self.default_config, **config}
        return self.default_config.copy()
    
    def generate_backtest_script(self, expert_name: str, config: Dict[str, Any]) -> str:
        """Generate MQL5 script for running backtests"""
        script = f"""
//+------------------------------------------------------------------+
//| Automated Backtest Script for {expert_name}                      |
//| Generated by Docker CI BacktestRunner                            |
//+------------------------------------------------------------------+
#property strict
#property script_show_inputs

// Backtest parameters
input string Symbol = "{config.get('symbol', 'EURUSD')}";
input ENUM_TIMEFRAMES Timeframe = {config.get('timeframe', 'PERIOD_H1')};
input datetime StartDate = D'{config.get('start_date', '2023.01.01')}';
input datetime EndDate = D'{config.get('end_date', '2023.12.31')}';
input double InitialDeposit = {config.get('initial_deposit', 10000)};
input int Leverage = {config.get('leverage', 100)};

//+------------------------------------------------------------------+
//| Script program start function                                    |
//+------------------------------------------------------------------+
void OnStart()
{{
    Print("Starting automated backtest for {expert_name}");
    Print("Symbol: ", Symbol);
    Print("Timeframe: ", EnumToString(Timeframe));
    Print("Period: ", TimeToString(StartDate), " to ", TimeToString(EndDate));
    
    // Since we're in a headless Docker environment,
    // we would normally use MT5's terminal automation API
    // For now, we'll generate a configuration file that can be
    // used with terminal64.exe /config:backtest.ini
    
    string config_content = 
        "[Tester]\\n" +
        "Expert=" + "{expert_name}" + "\\n" +
        "Symbol=" + Symbol + "\\n" +
        "Period=" + EnumToString(Timeframe) + "\\n" +
        "FromDate=" + TimeToString(StartDate, TIME_DATE) + "\\n" +
        "ToDate=" + TimeToString(EndDate, TIME_DATE) + "\\n" +
        "Deposit=" + DoubleToString(InitialDeposit, 2) + "\\n" +
        "Leverage=1:" + IntegerToString(Leverage) + "\\n" +
        "ExecutionMode=0\\n" +  // Every tick
        "Optimization=0\\n" +
        "OptimizationCriterion=0\\n" +
        "ForwardMode=0\\n" +
        "Visual=0\\n";
    
    // Write configuration
    int file_handle = FileOpen("backtest_config.ini", FILE_WRITE|FILE_TXT|FILE_ANSI);
    if(file_handle != INVALID_HANDLE)
    {{
        FileWriteString(file_handle, config_content);
        FileClose(file_handle);
        Print("Backtest configuration saved to backtest_config.ini");
    }}
    else
    {{
        Print("ERROR: Failed to create backtest configuration file");
    }}
    
    Print("Backtest configuration complete");
}}
"""
        return script
    
    def run_backtest(self, expert_name: str, config: Dict[str, Any]) -> Dict[str, Any]:
        """Execute backtest and collect results"""
        print(f"üéØ Running backtest for {expert_name}")
        print(f"   Symbol: {config.get('symbol')}")
        print(f"   Period: {config.get('start_date')} to {config.get('end_date')}")
        
        start_time = time.time()
        
        # For now, simulate backtest execution
        # In real implementation, this would:
        # 1. Generate backtest script
        # 2. Run MT5 terminal with /config parameter
        # 3. Monitor execution
        # 4. Collect results from terminal's reports
        
        result = self._simulate_backtest(expert_name, config)
        
        duration = time.time() - start_time
        result['execution_time'] = duration
        result['timestamp'] = datetime.now().isoformat()
        
        # Save results
        result_file = self.results_dir / f"backtest_{expert_name}_{int(time.time())}.json"
        with open(result_file, 'w') as f:
            json.dump(result, f, indent=2)
        
        print(f"‚úÖ Backtest completed in {duration:.2f}s")
        print(f"   Net Profit: ${result.get('net_profit', 0):.2f}")
        print(f"   Win Rate: {result.get('win_rate', 0):.1f}%")
        
        return result
    
    def _simulate_backtest(self, expert_name: str, config: Dict[str, Any]) -> Dict[str, Any]:
        """Simulate backtest results (placeholder for actual MT5 backtesting)"""
        import random
        
        # Simulate realistic trading results
        total_trades = random.randint(50, 500)
        win_rate = random.uniform(45, 65)
        wins = int(total_trades * win_rate / 100)
        losses = total_trades - wins
        
        avg_win = random.uniform(50, 150)
        avg_loss = random.uniform(30, 100)
        
        gross_profit = wins * avg_win
        gross_loss = losses * avg_loss
        net_profit = gross_profit - gross_loss
        
        initial_deposit = config.get('initial_deposit', 10000)
        
        return {
            "expert": expert_name,
            "symbol": config.get('symbol'),
            "timeframe": config.get('timeframe'),
            "period": {
                "start": config.get('start_date'),
                "end": config.get('end_date')
            },
            "initial_deposit": initial_deposit,
            "final_balance": initial_deposit + net_profit,
            "net_profit": net_profit,
            "gross_profit": gross_profit,
            "gross_loss": gross_loss,
            "profit_factor": gross_profit / gross_loss if gross_loss > 0 else 0,
            "total_trades": total_trades,
            "winning_trades": wins,
            "losing_trades": losses,
            "win_rate": win_rate,
            "average_win": avg_win,
            "average_loss": avg_loss,
            "max_drawdown": abs(random.uniform(500, 2000)),
            "max_drawdown_percent": random.uniform(5, 20),
            "sharpe_ratio": random.uniform(0.5, 2.5),
            "recovery_factor": abs(net_profit / random.uniform(500, 2000)),
            "risk_reward_ratio": avg_win / avg_loss if avg_loss > 0 else 0
        }
    
    def analyze_results(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze backtest results for issues and performance metrics"""
        issues = []
        warnings = []
        recommendations = []
        
        # Check profitability
        if results['net_profit'] < 0:
            issues.append({
                "severity": "HIGH",
                "type": "UNPROFITABLE",
                "message": f"Strategy is losing money: ${results['net_profit']:.2f}",
                "recommendation": "Review entry/exit logic and risk management parameters"
            })
        
        # Check win rate
        if results['win_rate'] < 40:
            warnings.append({
                "severity": "MEDIUM",
                "type": "LOW_WIN_RATE",
                "message": f"Win rate is below 40%: {results['win_rate']:.1f}%",
                "recommendation": "Consider improving entry signals or trade filtering"
            })
        
        # Check profit factor
        if results['profit_factor'] < 1.5:
            warnings.append({
                "severity": "MEDIUM",
                "type": "LOW_PROFIT_FACTOR",
                "message": f"Profit factor below 1.5: {results['profit_factor']:.2f}",
                "recommendation": "Optimize risk/reward ratio and trade management"
            })
        
        # Check drawdown
        if results['max_drawdown_percent'] > 15:
            issues.append({
                "severity": "HIGH",
                "type": "HIGH_DRAWDOWN",
                "message": f"Maximum drawdown exceeds 15%: {results['max_drawdown_percent']:.1f}%",
                "recommendation": "Reduce position sizes and implement stricter risk controls"
            })
        
        # Check Sharpe ratio
        if results['sharpe_ratio'] < 1.0:
            warnings.append({
                "severity": "LOW",
                "type": "LOW_SHARPE",
                "message": f"Sharpe ratio below 1.0: {results['sharpe_ratio']:.2f}",
                "recommendation": "Strategy risk-adjusted returns could be improved"
            })
        
        # Generate overall assessment
        if not issues:
            if not warnings:
                status = "EXCELLENT"
            else:
                status = "GOOD"
        else:
            if len(issues) > 2:
                status = "CRITICAL"
            else:
                status = "NEEDS_IMPROVEMENT"
        
        return {
            "status": status,
            "issues": issues,
            "warnings": warnings,
            "recommendations": recommendations,
            "summary": {
                "total_issues": len(issues),
                "total_warnings": len(warnings),
                "performance_score": self._calculate_performance_score(results)
            }
        }
    
    def _calculate_performance_score(self, results: Dict[str, Any]) -> float:
        """Calculate overall performance score (0-100)"""
        score = 50.0  # Base score
        
        # Profitability (0-25 points)
        if results['net_profit'] > 0:
            roi = (results['net_profit'] / results['initial_deposit']) * 100
            score += min(25, roi / 4)  # Cap at 25 points (100% ROI = 25 points)
        
        # Win rate (0-20 points)
        score += min(20, (results['win_rate'] - 30) / 2)  # 30% = 0, 70% = 20
        
        # Profit factor (0-15 points)
        score += min(15, (results['profit_factor'] - 1) * 10)  # 1.0 = 0, 2.5 = 15
        
        # Drawdown (0-20 points, penalty for high drawdown)
        drawdown_penalty = results['max_drawdown_percent']
        score += max(-20, 20 - drawdown_penalty)
        
        # Sharpe ratio (0-20 points)
        score += min(20, results['sharpe_ratio'] * 8)  # 0.0 = 0, 2.5 = 20
        
        return max(0, min(100, score))


class PerformanceMonitor:
    """Monitors performance metrics and detects bottlenecks"""
    
    def __init__(self, results_dir: Path):
        self.results_dir = results_dir
    
    def collect_metrics(self, backtest_results: Dict[str, Any]) -> Dict[str, Any]:
        """Collect performance metrics from backtest"""
        return {
            "timestamp": datetime.now().isoformat(),
            "execution_time": backtest_results.get('execution_time', 0),
            "trades_per_second": backtest_results.get('total_trades', 0) / max(1, backtest_results.get('execution_time', 1)),
            "memory_usage_estimate": "N/A",  # Would come from actual MT5 execution
            "cpu_usage_estimate": "N/A"
        }
    
    def detect_bottlenecks(self, metrics: Dict[str, Any]) -> List[Dict[str, str]]:
        """Detect performance bottlenecks"""
        bottlenecks = []
        
        # Check execution time
        if metrics['execution_time'] > 300:  # 5 minutes
            bottlenecks.append({
                "type": "SLOW_EXECUTION",
                "severity": "MEDIUM",
                "message": f"Backtest took {metrics['execution_time']:.1f}s to execute",
                "recommendation": "Consider optimizing indicator calculations or reducing data points"
            })
        
        return bottlenecks


class FeedbackLoopManager:
    """Manages feedback loop for continuous improvement"""
    
    def __init__(self, results_dir: Path):
        self.results_dir = results_dir
        self.feedback_file = results_dir / "feedback_loop.json"
        self.history = self._load_history()
    
    def _load_history(self) -> List[Dict[str, Any]]:
        """Load historical feedback"""
        if self.feedback_file.exists():
            with open(self.feedback_file, 'r') as f:
                return json.load(f)
        return []
    
    def _save_history(self):
        """Save feedback history"""
        with open(self.feedback_file, 'w') as f:
            json.dump(self.history, f, indent=2)
    
    def add_feedback(self, results: Dict[str, Any], analysis: Dict[str, Any]):
        """Add new feedback entry"""
        entry = {
            "timestamp": datetime.now().isoformat(),
            "expert": results.get('expert'),
            "performance_score": analysis['summary']['performance_score'],
            "status": analysis['status'],
            "issues": analysis['issues'],
            "warnings": analysis['warnings'],
            "metrics": {
                "net_profit": results['net_profit'],
                "win_rate": results['win_rate'],
                "profit_factor": results['profit_factor'],
                "max_drawdown": results['max_drawdown_percent']
            }
        }
        
        self.history.append(entry)
        self._save_history()
    
    def get_trends(self) -> Dict[str, Any]:
        """Analyze trends in feedback history"""
        if len(self.history) < 2:
            return {"message": "Insufficient data for trend analysis"}
        
        recent = self.history[-5:]  # Last 5 runs
        
        # Calculate average performance score
        avg_score = sum(e['performance_score'] for e in recent) / len(recent)
        
        # Check if improving or degrading
        scores = [e['performance_score'] for e in recent]
        if len(scores) >= 3:
            trend = "IMPROVING" if scores[-1] > scores[-3] else "DEGRADING"
        else:
            trend = "STABLE"
        
        return {
            "average_performance_score": avg_score,
            "trend": trend,
            "recent_runs": len(recent),
            "total_runs": len(self.history)
        }
    
    def generate_report(self) -> str:
        """Generate feedback loop report"""
        trends = self.get_trends()
        
        report = f"""
# ProjectQuantum Backtest Feedback Loop Report

**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**Total Runs:** {trends.get('total_runs', 0)}

## Performance Trends

"""
        
        if self.history:
            latest = self.history[-1]
            report += f"""
### Latest Run
- **Expert:** {latest.get('expert')}
- **Status:** {latest.get('status')}
- **Performance Score:** {latest.get('performance_score', 0):.1f}/100
- **Net Profit:** ${latest['metrics']['net_profit']:.2f}
- **Win Rate:** {latest['metrics']['win_rate']:.1f}%
- **Profit Factor:** {latest['metrics']['profit_factor']:.2f}
- **Max Drawdown:** {latest['metrics']['max_drawdown']:.1f}%

### Trend Analysis
- **Average Score (last 5 runs):** {trends.get('average_performance_score', 0):.1f}/100
- **Trend:** {trends.get('trend', 'UNKNOWN')}

### Issues Detected
"""
            if latest.get('issues'):
                for issue in latest['issues']:
                    report += f"- **{issue['severity']}**: {issue['message']}\n"
                    report += f"  - Recommendation: {issue['recommendation']}\n"
            else:
                report += "- No critical issues detected ‚úÖ\n"
            
            report += "\n### Warnings\n"
            if latest.get('warnings'):
                for warning in latest['warnings']:
                    report += f"- **{warning['severity']}**: {warning['message']}\n"
            else:
                report += "- No warnings ‚úÖ\n"
        
        return report


def main():
    """Main entry point"""
    parser = argparse.ArgumentParser(
        description="ProjectQuantum Docker Backtesting Runner"
    )
    parser.add_argument(
        '--expert',
        default='Project_Quantum',
        help='Expert advisor name to backtest'
    )
    parser.add_argument(
        '--config',
        type=Path,
        help='Backtest configuration file (JSON)'
    )
    parser.add_argument(
        '--workspace',
        type=Path,
        default=Path('/workspace'),
        help='Workspace directory'
    )
    parser.add_argument(
        '--results',
        type=Path,
        default=Path('/results/backtests'),
        help='Results directory'
    )
    parser.add_argument(
        '--analyze-only',
        action='store_true',
        help='Only analyze existing results, do not run new backtest'
    )
    
    args = parser.parse_args()
    
    print("=" * 70)
    print("ProjectQuantum Docker Backtesting Runner")
    print("=" * 70)
    print()
    
    # Initialize components
    runner = BacktestRunner(args.workspace, args.results)
    monitor = PerformanceMonitor(args.results)
    feedback_loop = FeedbackLoopManager(args.results)
    
    if not args.analyze_only:
        # Load configuration
        config = runner.load_config(args.config)
        
        # Run backtest
        results = runner.run_backtest(args.expert, config)
        
        # Collect performance metrics
        perf_metrics = monitor.collect_metrics(results)
        
        # Analyze results
        analysis = runner.analyze_results(results)
        
        # Detect bottlenecks
        bottlenecks = monitor.detect_bottlenecks(perf_metrics)
        if bottlenecks:
            analysis['bottlenecks'] = bottlenecks
        
        # Add to feedback loop
        feedback_loop.add_feedback(results, analysis)
        
        # Print analysis
        print()
        print("=" * 70)
        print("BACKTEST ANALYSIS")
        print("=" * 70)
        print(f"Status: {analysis['status']}")
        print(f"Performance Score: {analysis['summary']['performance_score']:.1f}/100")
        print()
        
        if analysis['issues']:
            print("‚ùå ISSUES DETECTED:")
            for issue in analysis['issues']:
                print(f"  [{issue['severity']}] {issue['message']}")
                print(f"    ‚Üí {issue['recommendation']}")
            print()
        
        if analysis['warnings']:
            print("‚ö†Ô∏è  WARNINGS:")
            for warning in analysis['warnings']:
                print(f"  [{warning['severity']}] {warning['message']}")
            print()
        
        if bottlenecks:
            print("üêå BOTTLENECKS:")
            for bottleneck in bottlenecks:
                print(f"  [{bottleneck['severity']}] {bottleneck['message']}")
                print(f"    ‚Üí {bottleneck['recommendation']}")
            print()
    
    # Generate and display feedback loop report
    report = feedback_loop.generate_report()
    
    report_file = args.results / "feedback_report.md"
    with open(report_file, 'w') as f:
        f.write(report)
    
    print()
    print("=" * 70)
    print("FEEDBACK LOOP REPORT")
    print("=" * 70)
    print(report)
    
    # Determine exit code based on analysis
    if not args.analyze_only:
        if analysis['status'] in ['CRITICAL', 'NEEDS_IMPROVEMENT']:
            print()
            print("‚ö†Ô∏è  Backtest did not meet quality standards")
            sys.exit(1)
    
    print()
    print("‚úÖ Backtesting complete")
    sys.exit(0)


if __name__ == '__main__':
    main()
