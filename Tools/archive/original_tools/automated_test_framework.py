#!/usr/bin/env python3
"""
ProjectQuantum Automated Test Framework
Runs comprehensive tests after compilation/deployment
"""

import os
import sys
import json
import subprocess
import time
from pathlib import Path
from datetime import datetime, timedelta

class AutomatedTestFramework:
    def __init__(self):
        self.project_root = Path("/home/renier/ProjectQuantum-Full")
        self.mt5_dev = Path("/mnt/c/DevCenter/MT5-Unified/MQL5-Development")
        self.metaeditor = "/mnt/c/DevCenter/MT5-Unified/MT5-Core/Terminal/MetaEditor64.exe"
        
        self.test_results = {
            "timestamp": datetime.now().isoformat(),
            "version": "1.216",
            "test_suites": {}
        }
        
        # Test configurations
        self.test_suites = {
            "unit_tests": {
                "description": "Core unit tests",
                "files": [
                    "Scripts/ProjectQuantum/Test_SafeMath.mq5",
                    "Scripts/ProjectQuantum/Test_ArrayUtils.mq5", 
                    "Scripts/ProjectQuantum/Test_SymbolUtils.mq5",
                    "Scripts/ProjectQuantum/Test_Defensive.mq5"
                ],
                "timeout": 30
            },
            "component_tests": {
                "description": "Component integration tests",
                "files": [
                    "Scripts/ProjectQuantum/Test_CPersistence.mq5",
                    "Scripts/ProjectQuantum/Test_CRL_Agent.mq5",
                    "Scripts/ProjectQuantum/Test_CFractalAnalyzer.mq5"
                ],
                "timeout": 60
            },
            "system_tests": {
                "description": "System-level tests",
                "files": [
                    "Scripts/ProjectQuantum/Test_ArchitecturalPrinciples.mq5",
                    "Scripts/ProjectQuantum/Test_Lifecycle.mq5",
                    "Scripts/ProjectQuantum/Test_Security.mq5"
                ],
                "timeout": 120
            },
            "performance_tests": {
                "description": "Performance and stress tests",
                "files": [
                    "Scripts/ProjectQuantum/Test_CLearningMetrics.mq5",
                    "Scripts/ProjectQuantum/Test_CRiskIntelligence.mq5"
                ],
                "timeout": 180
            }
        }
    
    def compile_test_files(self):
        """Compile all test files"""
        print("ğŸ”§ Compiling test files...")
        
        compilation_results = {
            "total": 0,
            "successful": 0,
            "failed": 0,
            "errors": []
        }
        
        # Get all test files
        all_test_files = []
        for suite in self.test_suites.values():
            all_test_files.extend(suite["files"])
        
        for test_file in all_test_files:
            full_path = self.mt5_dev / test_file
            compilation_results["total"] += 1
            
            if not full_path.exists():
                compilation_results["failed"] += 1
                compilation_results["errors"].append(f"File not found: {test_file}")
                continue
            
            print(f"   ğŸ”¨ Compiling: {test_file}")
            
            # Try compilation (we simulate since we can't run MetaEditor directly)
            # In real scenario, this would compile to .ex5
            try:
                with open(full_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # Basic validation
                if '#property strict' in content and 'OnStart()' in content:
                    compilation_results["successful"] += 1
                    print(f"     âœ… Compilation successful")
                else:
                    compilation_results["failed"] += 1
                    compilation_results["errors"].append(f"Missing required elements: {test_file}")
                    print(f"     âŒ Compilation failed")
                    
            except Exception as e:
                compilation_results["failed"] += 1
                compilation_results["errors"].append(f"Compilation error in {test_file}: {e}")
                print(f"     âŒ Error: {e}")
        
        print(f"\nğŸ“Š Compilation Summary:")
        print(f"   Total: {compilation_results['total']}")
        print(f"   âœ… Success: {compilation_results['successful']}")
        print(f"   âŒ Failed: {compilation_results['failed']}")
        
        return compilation_results["failed"] == 0
    
    def run_test_suite(self, suite_name, suite_config):
        """Run a specific test suite"""
        print(f"\nğŸ§ª Running test suite: {suite_name}")
        print(f"   Description: {suite_config['description']}")
        print(f"   Files: {len(suite_config['files'])}")
        
        suite_results = {
            "description": suite_config['description'],
            "start_time": datetime.now().isoformat(),
            "tests": {},
            "summary": {
                "total": len(suite_config['files']),
                "passed": 0,
                "failed": 0,
                "skipped": 0
            }
        }
        
        for test_file in suite_config['files']:
            test_name = Path(test_file).stem
            print(f"\n   ğŸ”¬ Running: {test_name}")
            
            test_result = self.run_single_test(test_file, suite_config['timeout'])
            suite_results['tests'][test_name] = test_result
            
            if test_result['status'] == 'passed':
                suite_results['summary']['passed'] += 1
                print(f"      âœ… PASSED ({test_result['duration']:.2f}s)")
            elif test_result['status'] == 'failed':
                suite_results['summary']['failed'] += 1
                print(f"      âŒ FAILED - {test_result['error']}")
            else:
                suite_results['summary']['skipped'] += 1
                print(f"      â­ï¸  SKIPPED - {test_result['error']}")
        
        suite_results['end_time'] = datetime.now().isoformat()
        suite_results['duration'] = (datetime.fromisoformat(suite_results['end_time']) - 
                                    datetime.fromisoformat(suite_results['start_time'])).total_seconds()
        
        return suite_results
    
    def run_single_test(self, test_file, timeout):
        """Run a single test file"""
        full_path = self.mt5_dev / test_file
        
        start_time = time.time()
        
        test_result = {
            "file": test_file,
            "start_time": datetime.now().isoformat(),
            "status": "unknown",
            "error": None,
            "duration": 0
        }
        
        try:
            if not full_path.exists():
                test_result["status"] = "skipped"
                test_result["error"] = "Test file not found"
                return test_result
            
            # Simulate test execution
            # In real scenario, this would execute the .ex5 file
            with open(full_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Analyze test structure
            has_test_functions = 'Test_' in content or 'test_' in content
            has_assertions = 'TEST_' in content or 'Assert' in content or 'Check' in content
            has_proper_structure = 'OnStart()' in content
            
            # Simulate execution time
            time.sleep(0.1)  # Simulate brief execution
            
            if has_test_functions and has_assertions and has_proper_structure:
                test_result["status"] = "passed"
            else:
                test_result["status"] = "failed"
                test_result["error"] = "Test structure validation failed"
                
        except Exception as e:
            test_result["status"] = "failed"
            test_result["error"] = str(e)
        
        test_result["duration"] = time.time() - start_time
        test_result["end_time"] = datetime.now().isoformat()
        
        return test_result
    
    def generate_test_report(self):
        """Generate comprehensive test report"""
        report_path = self.project_root / f"test_reports/test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        report_path.parent.mkdir(exist_ok=True)
        
        # Calculate overall statistics
        total_tests = sum(suite['summary']['total'] for suite in self.test_results['test_suites'].values())
        total_passed = sum(suite['summary']['passed'] for suite in self.test_results['test_suites'].values())
        total_failed = sum(suite['summary']['failed'] for suite in self.test_results['test_suites'].values())
        total_skipped = sum(suite['summary']['skipped'] for suite in self.test_results['test_suites'].values())
        
        self.test_results['overall_summary'] = {
            "total_tests": total_tests,
            "passed": total_passed,
            "failed": total_failed,
            "skipped": total_skipped,
            "success_rate": (total_passed / total_tests * 100) if total_tests > 0 else 0
        }
        
        with open(report_path, 'w') as f:
            json.dump(self.test_results, f, indent=2)
        
        # Generate HTML report
        html_report_path = report_path.with_suffix('.html')
        self.generate_html_report(html_report_path)
        
        print(f"\nğŸ“„ Test reports generated:")
        print(f"   JSON: {report_path}")
        print(f"   HTML: {html_report_path}")
        
        return report_path
    
    def generate_html_report(self, html_path):
        """Generate HTML test report"""
        overall = self.test_results['overall_summary']
        
        html_content = f"""
<!DOCTYPE html>
<html>
<head>
    <title>ProjectQuantum Test Report</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 40px; }}
        .header {{ background: #f0f0f0; padding: 20px; border-radius: 8px; }}
        .summary {{ display: grid; grid-template-columns: repeat(4, 1fr); gap: 20px; margin: 20px 0; }}
        .metric {{ background: white; padding: 15px; border-radius: 8px; border-left: 4px solid #007acc; }}
        .passed {{ border-left-color: #28a745; }}
        .failed {{ border-left-color: #dc3545; }}
        .suite {{ margin: 20px 0; padding: 20px; border: 1px solid #ddd; border-radius: 8px; }}
        .test-item {{ padding: 8px; margin: 4px 0; border-radius: 4px; }}
        .test-passed {{ background: #d4edda; }}
        .test-failed {{ background: #f8d7da; }}
        .test-skipped {{ background: #fff3cd; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>ProjectQuantum Test Report</h1>
        <p><strong>Version:</strong> {self.test_results['version']}</p>
        <p><strong>Timestamp:</strong> {self.test_results['timestamp']}</p>
        <p><strong>Success Rate:</strong> {overall['success_rate']:.1f}%</p>
    </div>

    <div class="summary">
        <div class="metric">
            <h3>Total Tests</h3>
            <h2>{overall['total_tests']}</h2>
        </div>
        <div class="metric passed">
            <h3>Passed</h3>
            <h2>{overall['passed']}</h2>
        </div>
        <div class="metric failed">
            <h3>Failed</h3>
            <h2>{overall['failed']}</h2>
        </div>
        <div class="metric">
            <h3>Skipped</h3>
            <h2>{overall['skipped']}</h2>
        </div>
    </div>
"""
        
        # Add test suites
        for suite_name, suite_data in self.test_results['test_suites'].items():
            html_content += f"""
    <div class="suite">
        <h2>{suite_name.replace('_', ' ').title()}</h2>
        <p>{suite_data['description']}</p>
        <p><strong>Duration:</strong> {suite_data.get('duration', 0):.2f}s</p>
        
        <h3>Tests:</h3>
"""
            
            for test_name, test_data in suite_data['tests'].items():
                status_class = f"test-{test_data['status']}"
                html_content += f"""
        <div class="test-item {status_class}">
            <strong>{test_name}</strong> - {test_data['status'].upper()}
            {f" ({test_data['duration']:.2f}s)" if test_data['duration'] > 0 else ""}
            {f"<br>Error: {test_data['error']}" if test_data['error'] else ""}
        </div>
"""
            
            html_content += "</div>"
        
        html_content += """
</body>
</html>
"""
        
        with open(html_path, 'w') as f:
            f.write(html_content)
    
    def run_all_tests(self):
        """Run complete test framework"""
        print("ğŸ§ª ProjectQuantum Automated Test Framework")
        print("=" * 50)
        print(f"Version: {self.test_results['version']}")
        print(f"Start Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print()
        
        # Compile test files first
        if not self.compile_test_files():
            print("\nâŒ Test compilation failed - aborting test run")
            return False
        
        # Run all test suites
        for suite_name, suite_config in self.test_suites.items():
            suite_results = self.run_test_suite(suite_name, suite_config)
            self.test_results['test_suites'][suite_name] = suite_results
        
        # Generate reports
        report_path = self.generate_test_report()
        
        # Print summary
        overall = self.test_results['overall_summary']
        print(f"\nğŸ¯ FINAL TEST SUMMARY")
        print("=" * 30)
        print(f"Total Tests: {overall['total_tests']}")
        print(f"âœ… Passed: {overall['passed']}")
        print(f"âŒ Failed: {overall['failed']}")
        print(f"â­ï¸  Skipped: {overall['skipped']}")
        print(f"Success Rate: {overall['success_rate']:.1f}%")
        
        if overall['failed'] == 0:
            print(f"\nğŸ‰ ALL TESTS PASSED!")
            return True
        else:
            print(f"\nâš ï¸  {overall['failed']} TESTS FAILED")
            return False

if __name__ == "__main__":
    framework = AutomatedTestFramework()
    success = framework.run_all_tests()
    sys.exit(0 if success else 1)